# TIL ( 2020/12/28 )

- Forward propagation: Vectorized implementation

---

## Forward propagation: Vectorized implementation

아래 식은 이전시간에 봤던 신경망에 식이다. 

<p align="center"><img src="../image/Machine/12.23/004.PNG" style="zoom:30%;"/></p>

여기서 <a href="https://www.codecogs.com/eqnedit.php?latex=z^{(j)}_{k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z^{(j)}_{k}" title="z^{(j)}_{k}" /></a> 라는 변수를 통해서 다시 나타내게 되면 아래와 같은데 

<a href="https://www.codecogs.com/eqnedit.php?latex=\\&space;a_{1}^{(2)}=g(z_{1}^{(2)})\\&space;a_{2}^{(2)}=g(z_{2}^{(2)})\\&space;a_{3}^{(2)}=g(z_{3}^{(2)})" target="_blank" ><img src="https://latex.codecogs.com/gif.latex?\\&space;a_{1}^{(2)}=g(z_{1}^{(2)})\\&space;a_{2}^{(2)}=g(z_{2}^{(2)})\\&space;a_{3}^{(2)}=g(z_{3}^{(2)})" title="\\ a_{1}^{(2)}=g(z_{1}^{(2)})\\ a_{2}^{(2)}=g(z_{2}^{(2)})\\ a_{3}^{(2)}=g(z_{3}^{(2)})" /></a>

  

위에서 <a href="https://www.codecogs.com/eqnedit.php?latex=z^{(j)}_{k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z^{(j)}_{k}" title="z^{(j)}_{k}" /></a> 라는 변수가 의미하는 것은 j는 layer로 만약 j=2일때를 보면 아래와 같은 의미를 갖는다. 

<a href="https://www.codecogs.com/eqnedit.php?latex=z_{k}^{(2)}=\Theta&space;_{k,0}^{(1)}x_{0}&plus;\Theta&space;_{k,1}^{(1)}x_{1}&space;&plus;\cdots&space;&plus;&space;\Theta&space;_{k,n}^{(1)}x_{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z_{k}^{(2)}=\Theta&space;_{k,0}^{(1)}x_{0}&plus;\Theta&space;_{k,1}^{(1)}x_{1}&space;&plus;\cdots&space;&plus;&space;\Theta&space;_{k,n}^{(1)}x_{n}" title="z_{k}^{(2)}=\Theta _{k,0}^{(1)}x_{0}+\Theta _{k,1}^{(1)}x_{1} +\cdots + \Theta _{k,n}^{(1)}x_{n}" /></a>

  

이제 x와 z를 벡터로 간단히 표현해보자

<a href="https://www.codecogs.com/eqnedit.php?latex=x=\begin{bmatrix}&space;x_{0}\\&space;x_{1}\\&space;\cdots&space;\\&space;x_{n}&space;\end{bmatrix}&space;z^{(j)}=\begin{bmatrix}&space;z_{1}^{(j)}\\&space;z_{2}^{(j)}\\&space;\cdots&space;\\&space;z_{n}^{(j)}&space;\end{bmatrix}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x=\begin{bmatrix}&space;x_{0}\\&space;x_{1}\\&space;\cdots&space;\\&space;x_{n}&space;\end{bmatrix}&space;z^{(j)}=\begin{bmatrix}&space;z_{1}^{(j)}\\&space;z_{2}^{(j)}\\&space;\cdots&space;\\&space;z_{n}^{(j)}&space;\end{bmatrix}" title="x=\begin{bmatrix} x_{0}\\ x_{1}\\ \cdots \\ x_{n} \end{bmatrix} z^{(j)}=\begin{bmatrix} z_{1}^{(j)}\\ z_{2}^{(j)}\\ \cdots \\ z_{n}^{(j)} \end{bmatrix}" /></a>

  

이때 <a href="https://www.codecogs.com/eqnedit.php?latex=x=a^{(1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x=a^{(1)}" title="x=a^{(1)}" /></a> 로 두면 아래와 같이 다시 식을 만들수 있다.

<a href="https://www.codecogs.com/eqnedit.php?latex=z^{(j)}=\Theta&space;^{(j-1)}a^{(j-1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z^{(j)}=\Theta&space;^{(j-1)}a^{(j-1)}" title="z^{(j)}=\Theta ^{(j-1)}a^{(j-1)}" /></a>

  

---

  

강의를 보면 구구절절 말이 많지만 가장 중요한것은 각 node들이 mapping되어 계산되는 부분들을 위와 같이 <a href="https://www.codecogs.com/eqnedit.php?latex=z^{(j)}_{k}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z^{(j)}_{k}" title="z^{(j)}_{k}" /></a> 라는 변수를 통해서 간단한 식으로 표현 할 수 있고 j = 1,2...n까지의 layer를 갖고 있을때 마지막에서 본 식과 같이 j번째 이전 레이어와 같이 계산이 되며 쌓여서 최종적으로 output이 되는 layer에서는 단 하나의 행을 갖기 때문에 single number가 되고 식은  <a href="https://www.codecogs.com/eqnedit.php?latex=h_{\Theta&space;}(x)=a^{(j&plus;1)}=g(z^{(j&plus;1)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?h_{\Theta&space;}(x)=a^{(j&plus;1)}=g(z^{(j&plus;1)})" title="h_{\Theta }(x)=a^{(j+1)}=g(z^{(j+1)})" /></a> 가 된다는것을 명심하자.

  

마지막 단계에서 Layer j와 j+1사이에서 logistic regression에서 했던 것과 같은 방법이 사용되는것을 주목해야 한다. 그리고 이렇게 layer를 쌓아 올림으로써 더욱 복잡하고 흥미로운 non-linear hypothesis를 만들어 낼 수 있다.

  



>## Reference

- https://www.coursera.org/learn/machine-learning

