# TIL ( 2021/01/11 )

- Backpropagation Algorithm

---

이전 시간에 봤던 NN에서 사용하는 Cost function을 최소화 시키기 위해서는 매개변수 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta" title="\Theta" /></a> 를 찾아야 하고 gradient descent나 다른 최적화 알고리즘을 사용해야 한다. 그래서 우리가 해야하는것은 매개변수 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta" title="\Theta" /></a> 를 이용해서 <a href="https://www.codecogs.com/eqnedit.php?latex=J(\Theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J(\Theta)" title="J(\Theta)" /></a>를 계산하고 이것을 편미분해야한다.

## Backpropagation

먼저 하나의 <a href="https://www.codecogs.com/eqnedit.php?latex=(x,y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(x,y)" title="(x,y)" /></a> training sample이 있는 경우 계산되는 과정을 살펴 보면 입력 x에 대한 출력을 계산하기 위해 Forward propagation을 시키게 된다. 

 <p align="center"><img src="../image/Machine/01.11/001.PNG" style="zoom:50%;"/></p>

여기서 <a href="https://www.codecogs.com/eqnedit.php?latex=a^{(1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(1)}" title="a^{(1)}" /></a> 은 첫번째 layer의 input에 해당하고 차례대로 <a href="https://www.codecogs.com/eqnedit.php?latex=a^{(2)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(2)}" title="a^{(2)}" /></a> 를 거치면서 sigmoid를 적용하고 bias를 더하는 과정을 이어가고 최종적으로 hypothesis 가 나오게된다. 이런 과정에 forward propagation이다. 

  

본론으로 돌아가 back propagation은 각 노드에 대해 <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{j}^{(l)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{j}^{(l)}" title="\delta _{j}^{(l)}" /></a> 을 구하는것인데 여기서 <a href="https://www.codecogs.com/eqnedit.php?latex=l" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l" title="l" /></a>은 레이어를 뜻하고 <a href="https://www.codecogs.com/eqnedit.php?latex=j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?j" title="j" /></a>는 해당 레이어의 노드에 에러가 있는지 나타낸다. 다시 정리해서 <a href="https://www.codecogs.com/eqnedit.php?latex=a^{(l)}_{j}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(l)}_{j}" title="a^{(l)}_{j}" /></a> 는 해당 node를 활성화 시키는것을 의미하는 반면에 <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;_{j}^{(l)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;_{j}^{(l)}" title="\delta _{j}^{(l)}" /></a> 는 활성화된 노드에 에러를 계산하는 것이다. Layer가 4개인 경우에는 아래와 같이 back propagation을 정의 할 수있다. 

 <p align="center"><img src="../image/Machine/01.11/002.PNG" style="zoom:50%;"/></p>

위에 back propagation 식에서 input layer의 항이 없는 것을 볼 수 있는데 input layer는 training sample에 feature 값이기 때문에 에러값이 존재하지 않기 때문이다. 



## Backpropagation Algorithm

이제 하나의 training sample이 아닌 여러개의 training sample을 사용한다 했을때 Algorithm을 보면

- Set <a href="https://www.codecogs.com/eqnedit.php?latex=\bigtriangleup&space;_{i,j}^{(l)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\;\Delta _{i,j}^{(l)}" title="\bigtriangleup _{i,j}^{(l)}" /></a> := 0 for all(l,i,j)
- Set <a href="https://www.codecogs.com/eqnedit.php?latex=a^{(1)}:=x^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(1)}:=x^{(t)}" title="a^{(1)}:=x^{(t)}" /></a> (training sample t=1 to m)
- Perform forward propagation 
- Using <a href="https://www.codecogs.com/eqnedit.php?latex=y^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y^{(t)}" title="y^{(t)}" /></a> , Compute <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(L)}=a^{(L)}-y^{(t)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(L)}=a^{(L)}-y^{(t)}" title="\delta ^{(L)}=a^{(L)}-y^{(t)}" /></a>
- Compute <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(L-1)},\delta&space;^{(L-2)}...\delta&space;^{(2)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(L-1)},\delta&space;^{(L-2)}...\delta&space;^{(2)}" title="\delta ^{(L-1)},\delta ^{(L-2)}...\delta ^{(2)}" /></a> using <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(l)}=((\Theta&space;^{(l)})^{T}\delta^{(l&plus;1)}).*a^{(l)}.*(1-a^{(l)})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(l)}=((\Theta&space;^{(l)})^{T}\delta^{(l&plus;1)}).*a^{(l)}.*(1-a^{(l)})" title="\delta ^{(l)}=((\Theta ^{(l)})^{T}\delta^{(l+1)}).*a^{(l)}.*(1-a^{(l)})" /></a> 
- <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;_{i,j}^{(l)}:=\Delta&space;_{i,j}^{(l)}&plus;a_{j}^{(l)}\delta_{i}^{(l&plus;1)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;_{i,j}^{(l)}:=\Delta&space;_{i,j}^{(l)}&plus;a_{j}^{(l)}\delta_{i}^{(l&plus;1)}" title="\Delta _{i,j}^{(l)}:=\Delta _{i,j}^{(l)}+a_{j}^{(l)}\delta_{i}^{(l+1)}" /></a> or with vectorization,  <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta&space;^{(l)}:=\Delta&space;^{(l)}&plus;\delta^{(l&plus;1)}(a^{(l)})^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta&space;^{(l)}:=\Delta&space;^{(l)}&plus;\delta^{(l&plus;1)}(a^{(l)})^{T}" title="\Delta ^{(l)}:=\Delta ^{(l)}+\delta^{(l+1)}(a^{(l)})^{T}" /></a>
- Hence we update our new <a href="https://www.codecogs.com/eqnedit.php?latex=\Delta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Delta" title="\Delta" /></a> matrix.
  - <a href="https://www.codecogs.com/eqnedit.php?latex=D^{(l)}_{i,j}:=\frac{1}{m}(\Delta_{i,j}^{(l)}&plus;\lambda&space;\Theta&space;_{i,j}^{(l)})&space;(j\neq&space;0)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D^{(l)}_{i,j}:=\frac{1}{m}(\Delta_{i,j}^{(l)}&plus;\lambda&space;\Theta&space;_{i,j}^{(l)})&space;(j\neq&space;0)" title="D^{(l)}_{i,j}:=\frac{1}{m}(\Delta_{i,j}^{(l)}+\lambda \Theta _{i,j}^{(l)}) (j\neq 0)" /></a>
  - <a href="https://www.codecogs.com/eqnedit.php?latex=D^{(l)}_{i,j}:=\frac{1}{m}\Delta_{i,j}^{(l)}&space;(j=&space;0)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D^{(l)}_{i,j}:=\frac{1}{m}\Delta_{i,j}^{(l)}&space;(j=&space;0)" title="D^{(l)}_{i,j}:=\frac{1}{m}\Delta_{i,j}^{(l)} (j= 0)" /></a>

- D matrix는 accumulator로서 우리가 구한 값들을 더하고 궁극적으로 편미분을 구하는 데 이용된다. 

<a href="https://www.codecogs.com/eqnedit.php?latex=D^{(l)}_{i,j}=\frac{\partial&space;J(\Theta&space;)}{\partial&space;\Theta&space;_{i,j}^{(l)}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D^{(l)}_{i,j}=\frac{\partial&space;J(\Theta&space;)}{\partial&space;\Theta&space;_{i,j}^{(l)}}" title="D^{(l)}_{i,j}=\frac{\partial J(\Theta )}{\partial \Theta _{i,j}^{(l)}}" /></a>



>## Reference

- https://www.coursera.org/learn/machine-learning

