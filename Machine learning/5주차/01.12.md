# TIL ( 2021/01/12 )

- Backpropagation Intuition

---

## Backpropagation Intuition

Backpropagation을 이해하기 위해서 2개의 hidden layer가 있는 network를 예로 Forward Propagation을 다시 보면

 <p align="center"><img src="../image/Machine/01.12/001.PNG" style="zoom:50%;"/></p>

첫번째 hidden layer에서는 입력값에 의해서 <a href="https://www.codecogs.com/eqnedit.php?latex=z^{(2)}_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z^{(2)}_1" title="z^{(2)}_1" /></a> 과 <a href="https://www.codecogs.com/eqnedit.php?latex=z^{(2)}_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z^{(2)}_2" title="z^{(2)}_2" /></a> 가 계산된다. 이것은 가중치의 합계를 뜻하고 그 다음 activation function을 적용하면 <a href="https://www.codecogs.com/eqnedit.php?latex=a^{(2)}_1" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(2)}_1" title="a^{(2)}_1" /></a> 와 <a href="https://www.codecogs.com/eqnedit.php?latex=a^{(2)}_2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{(2)}_2" title="a^{(2)}_2" /></a> 를 구할수있다. 그럼 이 값에 각각의 가중치를 곱해주고 더해준뒤에 다시 두번째 hidden layer에 전달하여 똑같은 과정을 반복하여 최종 output 값을 얻어낼수있다. 

  

이제 실제 Backpropagation의 수행과정을 살펴보자 이전시간에도 말했듯 우리는 backpropagation을 <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(l)}_j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(l)}_j" title="\delta ^{(l)}_j" /></a> 로 표현하고 이것은 <a href="https://www.codecogs.com/eqnedit.php?latex=l" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l" title="l" /></a>번째 layer에 unit <a href="https://www.codecogs.com/eqnedit.php?latex=j" target="_blank"><img src="https://latex.codecogs.com/gif.latex?j" title="j" /></a>의 활성화 값에 대한 error로 생각할 수 있다.

  <p align="center"><img src="../image/Machine/01.12/002.PNG" style="zoom:50%;"/></p>

그럼 위에 Forward Propagation의 예제를 다시 가져와서 Backpropagation을 하면 아래와 같은 그림으로 표현할 수 있는데, 

 <p align="center"><img src="../image/Machine/01.12/003.PNG" style="zoom:70%;"/></p>

복잡하다고 생각할 수 있지만 <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(2)}_{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(2)}_{2}" title="\delta ^{(2)}_{2}" /></a> 를 예로 보자 <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(2)}_{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(2)}_{2}" title="\delta ^{(2)}_{2}" /></a> 는 어떻게 계산되는지 생각해보면 그 식은 이렇게 정의할 수 있다. <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(2)}_{2}=\Theta&space;_{12}^{(2)}\delta&space;_{1}^{(3)}&plus;\Theta&space;_{22}^{(2)}\delta&space;_{2}^{(3)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(2)}_{2}=\Theta&space;_{12}^{(2)}\delta&space;_{1}^{(3)}&plus;\Theta&space;_{22}^{(2)}\delta&space;_{2}^{(3)}" title="\delta ^{(2)}_{2}=\Theta _{12}^{(2)}\delta _{1}^{(3)}+\Theta _{22}^{(2)}\delta _{2}^{(3)}" /></a> 이것만으로 이해가 안될수 있으니 다른 예로 <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(3)}_{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(3)}_{2}" title="\delta ^{(3)}_{2}" /></a> 를 보자 <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(3)}_{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(3)}_{2}" title="\delta ^{(3)}_{2}" /></a> 는 마지막 output node와 하나만 연결된것을 볼 수 있는데 그럼 마지막 노드에 <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(4)}_{1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(4)}_{1}" title="\delta ^{(4)}_{1}" /></a> 와 가중치 <a href="https://www.codecogs.com/eqnedit.php?latex=\Theta&space;_{12}^{(3)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\Theta&space;_{12}^{(3)}" title="\Theta _{12}^{(3)}" /></a> 를 곱해주면  <a href="https://www.codecogs.com/eqnedit.php?latex=\delta&space;^{(3)}_{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta&space;^{(3)}_{2}" title="\delta ^{(3)}_{2}" /></a> 를 구할 수 있다. 



>## Reference

- https://www.coursera.org/learn/machine-learning

