# TIL ( 2021/01/05 )

- Cost function

---

이번 강의에서는 Logistic regression에 Cost function과 Multi-class classifiaction에서 Cost Function을 살펴본다. 또한 이 Cost function을 최소화 시키기위해 Backpropagation에 대해서도 알아본다. 

## Cost function

먼저 강의에서 사용하는 몇 가지 변수를 정의하면

- L : Total number of layers in the network
- <a href="https://www.codecogs.com/eqnedit.php?latex=s_{l}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?s_{l}" title="s_{l}" /></a> : number of units (not counting bias unit) in layer l
- K : number of output units/classes



기존에 Logistic regression에서는 아래와 같은 Cost function을 볼 수 있었는데 Neural network에서는 output으로 벡터가 나오게된다. 여기서 hypothesis는 k dimension 벡터이기 때문에  i는 신경망에 의해 출력되는 벡터의 i번째 요소를 뜻하게 된다. 

 <p align="center"><img src="../image/Machine/01.05/001.png" style="zoom:50%;"/></p>

위에 Neural network 식에는 multiple output node를 추가하기 위해 몇 가지 nested summation이 추가 되었다. 첫번째 항에 대괄호에는 output node의 수를 반복하는것이 들어가게 되고 이 부분에서 "K"가 추가된것이다.

 두번째 항은 regularization 부분인데, 복잡해 보이지만 theta matrix의 열(i)은 현재 layer의 node갯수와 같다. 또한 theta matrix의 행(j)은 다음 layer의 node 갯수와 같다고 보면된다. 그리고 logistic regression과 마찬가지로 모든 항에 제곱을 취한다. 

  

Note:

- the double sum simply adds up the logistic regression costs calculated for each cell in the output layer
- the triple sum simply adds up the squares of all the individual Θs in the entire network.
- the i in the triple sum does not refer to training example i

  



>## Reference

- https://www.coursera.org/learn/machine-learning

