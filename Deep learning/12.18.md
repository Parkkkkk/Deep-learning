# TIL ( 2020/12/18 )

- Gradient Descent
- Derivatives

---

오늘은 gradient descent algorithm을 사용해서 parameter w,b을 training 할 수 있는지 배운다.

## Gradient Descent

이전시간에 배운 Cost function은 parameter w,b가 얼마나 잘 설정이 되었는지를 보여주는데 w와 b를 적절하게 학습하기 위해서는 당연히 cost function을 최소화 시켜야한다.

<p align="center"><img src="../image/Deep_Learning/12.18/001.PNG" style="zoom:100%;"/></p>

Gradient Descent의 모습을 보면 위와 같은데 실제로는 더 큰 dimension일 수 있지만 임의로 w와 b가 실수라고 가정을 한다. Cost function은 convex하고 위에서 말했듯 우리는 J(cost function)이 최소화 되는 부분을 찾을 것이다. 파라미터의 적합한 값을 찾기전에 w와 b를 초기화 시켜야하는데 일반적으로는 0으로 초기화시키지만 랜덤하게 초기화 시키는 방법도 있다.

  

이제 아래의 예를 보자,

<p align="center"><img src="../image/Deep_Learning/12.18/002.jpg" style="zoom:50%;"/></p>

J라는 함수가 위와같이 있을때 우리는 w값을 반복적으로 update하면서 최소값에 도달할때까지 반복하게 된다. 여기서 알파는 learning rate로 한번에 얼만큼 진행할건지 조절하는것으로 뒤에서 더 자세히 다룬다. 그리고 파란색부분에 derivative는 update 또는 w에 얼마나 변화를 줄지 알려주는 값을 의미한다.  

  

왼쪽 그림을 보면 최소값에 도달하는데 오른쪽에 w가 초기값으로 주어졌을때 derivative는 당연히 양수의 값을 갖을 것이고 왼쪽에 w 있다면 음수의 값을 가지게 될것이다.  

  

## Derivatives





>## Reference

- https://www.coursera.org/learn/neural-networks-deep-learning

