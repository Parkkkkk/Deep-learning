# TIL ( 2020/12/17 )

- Logistic Regression Cost function

---

## Logistic Regression Cost function

Logistic Regression에서 Cost function은 머신러닝부분에서도 보았지만 복습하는 느낌으로 다시 보자

<p align="center"><img src="../image/Deep_Learning/12.17/001.PNG" style="zoom:100%;"/></p>

머신 러닝때 정의했던 수식들과 조금 기호가 다르기 때문에 조심해서 봐야한다. 결과값은 위와 같이 정의가 되는데 모델에 대한 파라미터를 학습 하기 위해서 이전 시간에 정의했던것과 같이 m개의 트레이닝 example로 이루어진 training set이 주어지고, w와 b를 찾아야한다.  

<p align="center"><img src="../image/Deep_Learning/12.17/002.PNG" style="zoom:100%;"/></p>

위에 그림에서 오른쪽 부분을 보면 이 식이 의미하는것은 우리가 예측한것과 GT와 비슷해야 한다는것이다(i는 i번째 example을 의미) .



## Loss function

뒤에서 자세히 다루기때문에 오늘 알아야 할것은 L이라는 loss함수는 true label y를 갖는 경우, 얼마나 정확히 결과값을 산출하는지 정의할때 사용한다는것이다. squared error loss를 사용하면 좋을것 같다고 생각 할 수 있지만 non-convex하기 때문에 gradient descent가 잘 되지 않느다는 문제가 있다. 그래서 Logistic regression에서는 아래와 같은 loss function을 사용한다. 

<p align="center"><img src="../image/Deep_Learning/12.17/003.PNG" style="zoom:100%;"/></p>

이 식이 왜 적합한지 두가지 예를 가지고 보자 

- y=1 인 경우, 식에서 두번째 항은 다 날아가고 앞에 첫뻔째 항만 남게 되는데 이럴 경우에는 예측한y의 값이 sigmoid이기 때문에 1을 넘지는 못하지만 최대한 큰 값을 갖도록 하는것이 좋다. 
- y=0 인 경우, 첫번째 항은 0이 되고 두번째 항만 남게 되는데 이럴 경우에는 예측한y의 값이 최대한 작은 값을 가져야 한다. 

이 두가지의 예가 이해가 안될 수 있지만 log를 그려서 해보면 한번해 이해가 될것이다. 

  

위에서는 loss function이 single traning example에 적용된것을 보여준거고 최종적으로 parameter의 cost를 나타내는 Coss function을 정의 하면 아래와 같다, 

<p align="center"><img src="../image/Deep_Learning/12.17/004.PNG" style="zoom:100%;"/></p>

그래서 우리는 w와 b를 찾고 cost function을 계속해서 줄이는것을 하면된다.

**혹시 여기에서 이해가 되지 않는다면 Machine learning쪽에서 다시 보기**







>## Reference

- https://www.coursera.org/learn/neural-networks-deep-learning

